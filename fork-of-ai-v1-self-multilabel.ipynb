{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pydicom\nfrom sklearn.utils import resample\nimport keras\nimport dask\nfrom keras.models import Sequential\nfrom keras.models import save_model\nfrom keras.layers import Dense\nfrom keras.layers import Activation\nfrom keras.layers import Flatten\nfrom keras.layers import Input\nfrom keras.layers import Concatenate\nfrom keras.applications.densenet import *\nfrom numbers import Number\nfrom keras.utils import to_categorical\nimport gc\nimport psutil\nfrom cachetools import TTLCache\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\nimport tensorflow as tf\n\n\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n        \ninput_filepath = \"../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/\"\ntrain_image_filepath = \"../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/stage_2_train/\"\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pydicom\nfrom pydicom.data import get_testdata_files\n\nprint(__doc__)\n\nfilename = train_image_filepath + \"ID_00019828f.dcm\"\ndataset = pydicom.dcmread(filename)\n\n# Normal mode:\nprint()\nprint(\"Filename.........:\", filename)\nprint()\n\nprint(\"Modality.........:\", dataset.Modality)\n\nif 'PixelData' in dataset:\n    rows = int(dataset.Rows)\n    cols = int(dataset.Columns)\n    print(\"Image size.......: {rows:d} x {cols:d}, {size:d} bytes\".format(\n        rows=rows, cols=cols, size=len(dataset.PixelData)))\n    if 'PixelSpacing' in dataset:\n        print(\"Pixel spacing....:\", dataset.PixelSpacing)\n\n# use .get() if not sure the item exists, and want a default value if missing\nprint(\"Slice location...:\", dataset.get('SliceLocation', \"(missing)\"))\n\n# plot the image using matplotlib\nplt.imshow(dataset.pixel_array, cmap=plt.cm.bone)\nplt.title('Before Windowing', y=-0.17)\nplt.savefig('before-windowing.png')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def brain_window(img):\n    \n    window_center =  img.WindowCenter if isinstance(img.WindowCenter, Number) else img.WindowCenter[0] \n    window_width = img.WindowWidth if isinstance(img.WindowWidth, Number) else img.WindowWidth[0] \n    slope, intercept  =  img.RescaleSlope, img.RescaleIntercept\n    img_min = window_center - window_width // 2\n    img_max = window_center + window_width // 2\n    img = img.pixel_array\n    img = img * dataset.RescaleSlope + intercept\n    img[img < img_min] = img_min\n    img[img > img_max] = img_max\n    # Normalize\n    img = (img - img_min) / (img_max - img_min)\n    return img\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(brain_window(dataset), cmap=plt.cm.bone)\nplt.title('After Windowing', y=-0.17)\nplt.savefig('after-windowing.png')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def balanced_subsample(x,y,subsample_size=1.0):\n\n    class_xs = []\n    min_elems = None\n\n    for yi in np.unique(y):\n        elems = [a for a,b in zip(x,y) if b == yi ]\n        class_xs.append((yi, elems))\n        if min_elems == None or len(elems) < min_elems:\n            min_elems = len(elems)\n\n    use_elems = min_elems\n    if subsample_size < 1:\n        use_elems = int(min_elems*subsample_size)\n\n    xs = []\n    ys = []\n\n    for ci,this_xs in class_xs:\n        if len(this_xs) > use_elems:\n            np.random.shuffle(this_xs)\n\n        x_ = this_xs[:use_elems]\n        y_ = np.empty(use_elems)\n        y_.fill(ci)\n\n        xs.extend(x_)\n        ys.extend(y_)\n\n    print (xs[:10], ys[:10])\n    return xs,ys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#########making y with all labels ################\n#####multilabel###############\nfilename = train_image_filepath + \"ID_00019828f.dcm\"\nfile_dcm = pydicom.dcmread(filename)\nprint(file_dcm.pixel_array.shape)\ndef get_two_class_labels(csv_file_path, stratify_percentage=1):\n    \"\"\"returns a list of tuples where the first value is the file id and the second is the label\n    [('ID_00019828f', 0)]\n    \"\"\"\n    \n    input_dataframe = pd.read_csv(csv_file_path)\n    #filtered_input_dataframe = input_dataframe[input_dataframe['ID'].apply(lambda x : 'any' in x) ]\n    files_with_ids = []\n    \n   # print(input_dataframe.columns.values)\n    X = list(input_dataframe['ID'])\n    y_dataframe = input_dataframe.drop(input_dataframe.columns[[0,1,7]], axis = 1)\n    \n  \n    #y = [y_dataframe.columns.values.tolist()] + y_dataframe.values.tolist()\n    y =  y_dataframe.values.tolist()\n    #print (y[0])\n    #print(len(X))\n    #print(len(y))\n    \n    num_samples = int(stratify_percentage * len(X))\n    print(\"Num Samples :\", num_samples)\n    \n    for k,v in list(zip(X, y)) :\n        files_with_ids.append( (\"_\".join(k.split('_')[:2]), v))\n        \n    return files_with_ids\n        \n    \n\ndef get_images(image_folder_root, image_label_list):\n    \"\"\"returns a list of tuples with ('ID',label,file) where file is the ndarray (with a readable shape )\"\"\"\n    file_dcm=[]\n    X = []\n    y = []\n    for file_name,label in image_label_list:\n        try:\n            current_file = pydicom.dcmread(image_folder_root + file_name + '.dcm')\n            pixel_array = current_file.pixel_array\n            if (pixel_array.shape != (512,512)):\n                continue\n            file_dcm.append((file_name,label,brain_window(current_file)))\n            y.append(label)\n            X.append(pydicom.dcmread(image_folder_root + file_name + '.dcm').pixel_array)\n        except ValueError:\n            continue\n    return X,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###using multilabel dataset\n#csv_file_path = \"../input/brain-ai/hem_positive_train_set.csv\"\ncsv_file_path = \"../input/brain-ai-equalsample/down_sampled_positive_data.csv\"\nimage_folder_root = train_image_filepath\nfiles_with_ids = get_two_class_labels(csv_file_path,stratify_percentage=1)\nX,y = [ x for x,y in files_with_ids], [y for x,y in files_with_ids]\nprint (len(files_with_ids))\nprint((y[0]))\nprint(X[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model():\n    \n    \n    def fit(self,X,y):\n        raise NotImplemetedError()\n    def predict(self, X):\n        \"\"\"Takes test data and returns the label probabilities \"\"\"\n        raise NotImplemetedError()\n\nclass Basic(Model):\n    \"\"\"intput dimension is the shape of the input\"\"\"\n    def __init__(self, input_dimension, output_dimension):\n        self.input_dimension = input_dimension\n        self.output_dimension = output_dimension\n        self.model = Sequential()\n        self.model.add(Flatten())\n        self.model.add(Dense(400,input_shape=(512,512)))\n        self.model.add(Activation('relu'))\n        self.model.add(Dense(200))\n        self.model.add(Activation('relu'))\n        self.model.add(Dense(50))\n        self.model.add(Activation('relu'))\n        self.model.add(Dense(25))\n        self.model.add(Activation('relu'))\n        self.model.add(Dense(10))\n        self.model.add(Activation('relu'))\n        self.model.add(Dense(5))\n        self.model.add(Activation('sigmoid'))\n        self.model.compile(optimizer='adam', loss='mean_squared_error',metrics=['accuracy'])\n\n \n        \n    \n    def fit(self, X,y):\n        self.model.fit(x=X,y=y,epochs=1,batch_size=8)\n    def predict(self, X):\n        self.model.predict(X)\n    def save(self,filename):\n        self.model.save(filename)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############multiclass classifier, basic model, with 5 neurons in the output layer, sigmoid function and categorical cross-entropy#########\nclass Basic(Model):\n    \"\"\"intput dimension is the shape of the input\"\"\"\n    def __init__(self, input_dimension, output_dimension):\n        self.input_dimension = input_dimension\n        self.output_dimension = output_dimension\n        self.model = Sequential()\n        self.model.add(Flatten())\n        self.model.add(Dense(400,input_shape=(512,512)))\n        self.model.add(Activation('relu'))\n        self.model.add(Dense(30))\n        self.model.add(Activation('relu'))\n        self.model.add(Dense(15))\n        self.model.add(Activation('relu'))\n        self.model.add(Dense(10))\n        self.model.add(Activation('relu'))\n        self.model.add(Dense(5))\n        self.model.add(Activation('sigmoid'))\n        self.model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n\n \n        \n    \n    def fit(self, X,y):\n        self.model.fit(x=X,y=y,epochs=1,batch_size=8)\n    def predict(self, X):\n        self.model.predict(X)\n        \n    #y=to_categorical(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Has an image loader for providing the image for a given id in the dataset \n#Written separately so that we can add any preprocessing steps here while the image is being loaded into memory\nclass DataLoader:\n    def __init__(self,base_file_path, cache_size=500, ttl_seconds=20):\n        self.base_file_path = base_file_path\n        self.cache = TTLCache(maxsize=cache_size,ttl=ttl_seconds)\n        \n        \n    ##will apply only brain windowing while loading the image for now. Need to change this to apply all windowing functions. \n    def load_image(self, image_id):\n        if image_id in self.cache:\n            return self.cache[image_id]\n        \n        else:\n            current_file = pydicom.dcmread(image_folder_root + image_id + '.dcm')\n            pixel_array = brain_window(current_file)\n            self.cache[image_id] = pixel_array\n            return pixel_array\n    def trigger_expire(self):\n        self.cache.expire()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class  ModelTrainer(object):\n    \n    def __init__(self, dataloader, training_batch_size=8, split_size = 400):\n        \n        self.dataloader = dataloader\n        self.split_size = split_size\n        \n    \n    \"\"\"Takes X and y as the file name and labels and \"\"\"    \n    def fit(self, X,y,model, epochs=10, training_batch_size=8 ):\n        splits = len(y) // self.split_size +1\n        \n        splitter = StratifiedKFold(n_splits=splits, random_state=None, shuffle=True)\n        count = 1\n        while epochs > 0:\n            X,y = shuffle(X,y)\n            print(\"Starting epoch \",count)\n            ##TODO: add a better split and shuffle mechanism\n            processed = 0\n            while processed < len(y):\n                batch_imgs = []\n                batch_labels = []\n            \n                current_x = X[processed:min(processed+self.split_size,len(y))]\n                current_y = y[processed:min(processed+self.split_size,len(y))]\n                for img,label in zip(current_x,current_y):\n                    image = self.dataloader.load_image(img)\n                    ##Figure out how many images are getting ignored because of this assumption\n                    ##check if all reshape operations can happen in the dataloader\n                    if image.shape != (512,512):\n                        continue\n                    batch_imgs.append(image)\n                    batch_labels.append(label)\n                print(\"using  batch with size\", len(batch_imgs), len(batch_labels), \"Processed \", processed, \"Total \", len(y))\n                model.fit(np.array(batch_imgs),np.array(batch_labels))\n                self.dataloader.trigger_expire()\n                del batch_imgs\n                del batch_labels\n                \n                gc.collect()\n                processed +=self.split_size\n            print(\"Ending Epoch\", count)\n            print(\"Saving Model\")\n            save_model(model.model,str(epochs)+\"-epoch-model-three.hdf5\")\n            #model.save(\"basic-model-1-epochs.h5\")\n            #model.model.save_model(model,h5pyBasic-model-1-epoch)\n            epochs-=1\n            count+=1\n        return model\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#m = keras.models.load_model(\"model.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#####MULTILABEL RUN#######\ndataloader = DataLoader(train_image_filepath)\nmodel = Basic(5,5)\ntrainer = ModelTrainer(dataloader,split_size=1000)\nmodel = trainer.fit(X,y,model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####calculating precsison, recall through keras itself\nprecision = tf.keras.metrics.Precision()\nprecision.update_state(y, y)\nprint('Final result: ', precision.result().numpy())  \n\nrecall = tf.keras.metrics.Recall()\nrecall.update_state(y, y)\nprint('Final result: ', recall.result().numpy())  \n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}