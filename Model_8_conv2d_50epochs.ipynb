{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pydicom\nfrom sklearn.utils import resample\nfrom keras.models import save_model\nimport keras\nimport dask\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Activation\nfrom keras.layers import Flatten\nfrom keras.layers import Input\nfrom keras.layers import Concatenate\nfrom keras.applications.densenet import *\nfrom numbers import Number\nfrom keras.utils import to_categorical\nimport gc\nimport psutil\nfrom cachetools import TTLCache\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils import shuffle\nimport tensorflow as tf\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers import Conv2D\n\n\n\n\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n        \ninput_filepath = \"../../rsna-intracranial-hemorrhage-detection/\"\ntrain_image_filepath = \"../../rsna-intracranial-hemorrhage-detection/stage_2_train/\"\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pydicom\nfrom pydicom.data import get_testdata_files\n\nprint(__doc__)\n\nfilename = train_image_filepath + \"ID_00019828f.dcm\"\ndataset = pydicom.dcmread(filename)\n\n# Normal mode:\nprint()\nprint(\"Filename.........:\", filename)\nprint()\n\nprint(\"Modality.........:\", dataset.Modality)\n\nif 'PixelData' in dataset:\n    rows = int(dataset.Rows)\n    cols = int(dataset.Columns)\n    print(\"Image size.......: {rows:d} x {cols:d}, {size:d} bytes\".format(\n        rows=rows, cols=cols, size=len(dataset.PixelData)))\n    if 'PixelSpacing' in dataset:\n        print(\"Pixel spacing....:\", dataset.PixelSpacing)\n\n# use .get() if not sure the item exists, and want a default value if missing\nprint(\"Slice location...:\", dataset.get('SliceLocation', \"(missing)\"))\n\n# plot the image using matplotlib\nplt.imshow(dataset.pixel_array, cmap=plt.cm.bone)\nplt.title('Before Windowing', y=-0.17)\nplt.savefig('before-windowing.png')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def brain_window(img):\n    \n    window_center =  img.WindowCenter if isinstance(img.WindowCenter, Number) else img.WindowCenter[0] \n    window_width = img.WindowWidth if isinstance(img.WindowWidth, Number) else img.WindowWidth[0] \n    slope, intercept  =  img.RescaleSlope, img.RescaleIntercept\n    img_min = window_center - window_width // 2\n    img_max = window_center + window_width // 2\n    img = img.pixel_array\n    img = img * dataset.RescaleSlope + intercept\n    img[img < img_min] = img_min\n    img[img > img_max] = img_max\n    # Normalize\n    img = (img - img_min) / (img_max - img_min)\n    return img\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(brain_window(dataset), cmap=plt.cm.bone)\nplt.title('After Windowing', y=-0.17)\nplt.savefig('after-windowing.png')\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def balanced_subsample(x,y,subsample_size=1.0):\n\n    class_xs = []\n    min_elems = None\n\n    for yi in np.unique(y):\n        elems = [a for a,b in zip(x,y) if b == yi ]\n        class_xs.append((yi, elems))\n        if min_elems == None or len(elems) < min_elems:\n            min_elems = len(elems)\n\n    use_elems = min_elems\n    if subsample_size < 1:\n        use_elems = int(min_elems*subsample_size)\n\n    xs = []\n    ys = []\n\n    for ci,this_xs in class_xs:\n        if len(this_xs) > use_elems:\n            np.random.shuffle(this_xs)\n\n        x_ = this_xs[:use_elems]\n        y_ = np.empty(use_elems)\n        y_.fill(ci)\n\n        xs.extend(x_)\n        ys.extend(y_)\n\n    print (xs[:10], ys[:10])\n    return xs,ys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#########making y with all labels ################\n#####multilabel###############\nfilename = train_image_filepath + \"ID_00019828f.dcm\"\nfile_dcm = pydicom.dcmread(filename)\nprint(file_dcm.pixel_array.shape)\ndef get_two_class_labels(csv_file_path, stratify_percentage=1):\n    \"\"\"returns a list of tuples where the first value is the file id and the second is the label\n    [('ID_00019828f', 0)]\n    \"\"\"\n    \n    input_dataframe = pd.read_csv(csv_file_path)\n    #filtered_input_dataframe = input_dataframe[input_dataframe['ID'].apply(lambda x : 'any' in x) ]\n    files_with_ids = []\n\n    \n   # print(input_dataframe.columns.values)\n    X = list(input_dataframe['ID'])\n    y_dataframe = input_dataframe.drop(input_dataframe.columns[[0,1,7]], axis = 1)\n\n    #print(y_dataframe.head)\n  \n    #y = [y_dataframe.columns.values.tolist()] + y_dataframe.values.tolist()\n    y =  y_dataframe.values.tolist()\n    #print (y[0])\n    #print(len(X))\n    #print(len(y))\n    \n    num_samples = int(stratify_percentage * len(X))\n    print(\"Num Samples :\", num_samples)\n    \n    for k,v in list(zip(X, y)) :\n        files_with_ids.append( (\"_\".join(k.split('_')[:2]), v))\n        \n    return files_with_ids\n        \n    \n\ndef get_images(image_folder_root, image_label_list):\n    \"\"\"returns a list of tuples with ('ID',label,file) where file is the ndarray (with a readable shape )\"\"\"\n    file_dcm=[]\n    X = []\n    y = []\n    for file_name,label in image_label_list:\n        try:\n            current_file = pydicom.dcmread(image_folder_root + file_name + '.dcm')\n            pixel_array = current_file.pixel_array\n            if (pixel_array.shape != (512,512)):\n                continue\n            file_dcm.append((file_name,label,brain_window(current_file)))\n            y.append(label)\n            X.append(pydicom.dcmread(image_folder_root + file_name + '.dcm').pixel_array)\n        except ValueError:\n            continue\n    return X,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#########making y with all labels ################\n#####multilabel###############\ndef get_two_class_labels_fortest(csv_file_path_test, stratify_percentage=1):\n    \"\"\"returns a list of tuples where the first value is the file id and the second is the label\n    [('ID_00019828f', 0)]\n    \"\"\"\n    \n    test_dataframe = pd.read_csv(csv_file_path_test)\n    #filtered_input_dataframe = input_dataframe[input_dataframe['ID'].apply(lambda x : 'any' in x) ]\n    files_with_ids_fortest = []\n    \n   # print(input_dataframe.columns.values)\n    X_test = list(test_dataframe['ID'])\n    \n    print(\"Testing sample\",X_test[0])\n    \n    y_test_df = test_dataframe.drop(test_dataframe.columns[[0,6]], axis = 1)\n                  \n    print(\"testing y samples\")\n    \n  \n    #y = [y_dataframe.columns.values.tolist()] + y_dataframe.values.tolist()\n    y_test = y_test_df.values.tolist()\n    #print (y[0])\n    #print(len(X))\n    #print(len(y))\n    \n    num_samples_train = int(stratify_percentage * len(X))\n    num_samples_test = int(stratify_percentage * len(X_test))\n    print(\"Num Samples in Training :\", num_samples_train)\n    print(\"Num Samples in Testing :\", num_samples_test)\n    \n    for k,v in list(zip(X_test, y_test)) :\n        files_with_ids_fortest.append( (\"_\".join(k.split('_')[:2]), v))\n        \n    return files_with_ids_fortest\n\ndef get_images(image_folder_root, image_label_list):\n    \"\"\"returns a list of tuples with ('ID',label,file) where file is the ndarray (with a readable shape )\"\"\"\n    file_dcm=[]\n    X_test = []\n    y_test = []\n    for file_name,label in image_label_list:\n        try:\n            current_file = pydicom.dcmread(image_folder_root + file_name + '.dcm')\n            pixel_array = current_file.pixel_array\n            if (pixel_array.shape != (512,512)):\n                continue\n            file_dcm.append((file_name,label,brain_window(current_file)))\n            y_test.append(label)\n            X_test.append(pydicom.dcmread(image_folder_root + file_name + '.dcm').pixel_array)\n        except ValueError:\n            continue\n    return X_test,y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###using multilabel dataset\n#csv_file_path = \"../input/brain-ai/hem_positive_train_set.csv\"\n#csv_file_path = \"../input/dsampled-positive-data/down_sampled_positive_data.csv\"\n#csv_file_path_test = \"../input/positive-test/hem_positive_test_set.csv\"\ncsv_file_path = \"./CSV/down_sampled_positive_data.csv\"\ncsv_file_path_test = \"./CSV/hem_positive_test_set.csv\"\nimage_folder_root = train_image_filepath\nfiles_with_ids = get_two_class_labels(csv_file_path,stratify_percentage=1)\nX,y = [ x for x,y in files_with_ids], [y for x,y in files_with_ids]\nprint (len(files_with_ids))\nprint((y[0]))\nprint(X[0])\n\nfiles_with_ids_fortest = get_two_class_labels_fortest(csv_file_path_test,stratify_percentage=1)\nX_test,y_test = [ x_test for x_test,y_test in files_with_ids_fortest], [y_test for x_test,y_test in files_with_ids_fortest]\nprint (len(files_with_ids_fortest))\nprint((y_test))\nprint(X_test[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model():\n    \n    \n    def fit(self,X,y):\n        raise NotImplemetedError()\n    def predict(self, X):\n        \"\"\"Takes test data and returns the label probabilities \"\"\"\n        raise NotImplemetedError()\n\nclass Basic(Model):\n    \"\"\"intput dimension is the shape of the input\"\"\"\n    def __init__(self, input_dimension, output_dimension):\n        self.input_dimension = input_dimension\n        self.output_dimension = output_dimension\n        self.model = Sequential()\n        self.model.add(Flatten())\n        self.model.add(Dense(400,input_shape=(512,512)))\n        self.model.add(LeakyReLU(alpha=0.3))\n        self.model.add(Dense(200))\n        self.model.add(LeakyReLU(alpha=0.3))\n        self.model.add(Dense(50))\n        self.model.add(LeakyReLU(alpha=0.3))\n        self.model.add(Dense(25))\n        self.model.add(LeakyReLU(alpha=0.3))\n        self.model.add(Dense(10))\n        self.model.add(LeakyReLU(alpha=0.3))\n        self.model.add(Dense(5))\n        self.model.add(Activation('sigmoid'))\n        self.model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n\n \n        \n    \n    def fit(self, X,y):\n        self.model.fit(x=X,y=y,epochs=1,batch_size=8)\n    def predict(self, X):\n        self.model.predict(X)\n    def save(self,filename):\n        self.model.save(filename)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############multiclass classifier, basic model, with 5 neurons in the output layer, sigmoid function and categorical cross-entropy#########\nclass Basic(Model):\n    \"\"\"intput dimension is the shape of the input\"\"\"\n    def __init__(self, input_dimension, output_dimension):\n        self.input_dimension = input_dimension\n        self.output_dimension = output_dimension\n        self.model = Sequential()\n     \n        #self.model.add(keras.layers.Conv2D(200,kernel_size=(3,3),data_format=None,activation='relu',input_shape=(700,512,512,1))) ###(batch, rows, cols, channels)\n        ###start conv2d layers###\n        self.model.add(Conv2D(64, kernel_size=3, activation='relu',input_shape=(512,512,1)))\n        #self.model.add(Conv2D(32, kernel_size=3, activation='relu'))\n        ###end conv2d layer###\n        self.model.add(Flatten())\n        self.model.add(Dense(400)) #,input_shape=(512,512)))\n        self.model.add(LeakyReLU(alpha=0.3))\n        self.model.add(Dense(200))\n        self.model.add(LeakyReLU(alpha=0.3))\n        self.model.add(Dense(100))\n        self.model.add(LeakyReLU(alpha=0.3))\n        #self.model.add(Activation('relu'))\n        self.model.add(Dense(30))\n        self.model.add(LeakyReLU(alpha=0.3))\n        #self.model.add(Activation('relu'))\n        self.model.add(Dense(15))\n        self.model.add(LeakyReLU(alpha=0.3))\n        #self.model.add(Activation('relu'))\n        self.model.add(Dense(10))\n        self.model.add(LeakyReLU(alpha=0.3))\n        #self.model.add(Activation('relu'))\n        self.model.add(Dense(5))\n        self.model.add(Activation('sigmoid'))\n        self.model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n\n \n        \n    \n    def fit(self, X,y):\n        self.model.fit(x=X,y=y,epochs=1,batch_size=8)\n    def predict(self, X):\n        pred = self.model.predict(X)\n        return pred\n        \n    #y=to_categorical(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"####TUNING LAYERS FOR MODEL#####\ndataloader = DataLoader(train_image_filepath)\nmodel = Basic(5,5)\ntrainer = ModelTrainer(dataloader,split_size=700)\nmodel = trainer.fit(X,y,model,epochs = 1)\nmodel.model.summary()"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Has an image loader for providing the image for a given id in the dataset \n#Written separately so that we can add any preprocessing steps here while the image is being loaded into memory\nclass DataLoader:\n    def __init__(self,base_file_path, cache_size=500, ttl_seconds=20):\n        self.base_file_path = base_file_path\n        self.cache = TTLCache(maxsize=cache_size,ttl=ttl_seconds)\n        \n        \n    ##will apply only brain windowing while loading the image for now. Need to change this to apply all windowing functions. \n    def load_image(self, image_id):\n        if image_id in self.cache:\n            return self.cache[image_id]\n        \n        else:\n            current_file = pydicom.dcmread(image_folder_root + image_id + '.dcm')\n            pixel_array = brain_window(current_file)\n            self.cache[image_id] = pixel_array\n            return pixel_array\n    def trigger_expire(self):\n        self.cache.expire()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class  ModelTrainer(object):\n    \n    def __init__(self, dataloader, training_batch_size=8, split_size = 400):\n        \n        self.dataloader = dataloader\n        self.split_size = split_size\n        \n    \n    \"\"\"Takes X and y as the file name and labels and \"\"\"    \n    def fit(self, X,y,model, epochs=5, training_batch_size=8):\n        splits = len(y) // self.split_size +1\n        \n        splitter = StratifiedKFold(n_splits=splits, random_state=None, shuffle=True)\n        count = 1\n        while epochs > 0:\n            X,y = shuffle(X,y)\n            print(\"Starting epoch \",count)\n            ##TODO: add a better split and shuffle mechanism\n            processed = 0\n            while processed < len(y):\n                batch_imgs = []\n                batch_labels = []\n            \n                current_x = X[processed:min(processed+self.split_size,len(y))]\n                current_y = y[processed:min(processed+self.split_size,len(y))]\n                for img,label in zip(current_x,current_y):\n                    image = self.dataloader.load_image(img)\n                    ##Figure out how many images are getting ignored because of this assumption\n                    ##check if all reshape operations can happen in the dataloader\n                    if image.shape != (512,512):\n                        continue\n                    batch_imgs.append(image)\n                    batch_labels.append(label)\n                print(\"Length of images\",len(batch_imgs))\n                print(\"Using  batch with size\", len(batch_imgs), len(batch_labels), \"Processed \", processed, \"Total \", len(y))\n                model.fit(np.array(batch_imgs),np.array(batch_labels))\n                #model.predict(np.array(batch_imgs),np.array(batch_labels))\n                self.dataloader.trigger_expire()\n                del batch_imgs\n                del batch_labels\n                \n                gc.collect()\n                processed +=self.split_size\n            print(\"Ending epoch\", count)\n            #save_model(model.model,\"epoch-model-four.hdf5\")\n            #model.save(\"basic-model-1-epochs.h5\")\n            #model.model.save_model(model,h5pyBasic-model-1-epoch)\n            epochs-=1\n            count+=1\n        \n                \n        return model\n    \n    def predict(self, X_test,y_test,model,epochs=5, training_batch_size=8 ):\n            \n            splits = len(y) // self.split_size +1\n            splitter = StratifiedKFold(n_splits=splits, random_state=None, shuffle=False)\n            processed = 0\n            while processed < len(y_test):\n                batch_imgs_test = []\n                batch_labels_test = []\n            \n                current_x_test = X_test[processed:min(processed+self.split_size,len(y_test))]\n                current_y_test = y_test[processed:min(processed+self.split_size,len(y_test))]\n                for img,label in zip(current_x_test,current_y_test):\n                    image = self.dataloader.load_image(img)\n                    ##Figure out how many images are getting ignored because of this assumption\n                    ##check if all reshape operations can happen in the dataloader\n                    if image.shape != (512,512):\n                        continue\n                    batch_imgs_test.append(image)\n                    batch_labels_test.append(label)\n                #print(\"Length of images\",len(batch_imgs_test))\n                #print(\"using  batch with size\", len(batch_imgs_test), len(batch_labels_test), \"Processed \", processed, \"Total \", len(y_test))\n\n                \n                batch_imgs_test = np.array(batch_imgs_test)\n               # print(\"Length of images\",len(batch_imgs_test))\n               # print(\"Shape before reshaping\",batch_imgs_test.shape)\n            \n            #predict_input = batch_imgs_test.reshape(len(X_test),2)\n            \n            #batch_imgs_test = np.expand_dims(batch_imgs_test, axis=1)\n            #batch_imgs_test = np.expand_dims(batch_imgs_test, axis=1)\n            \n                #print(\"shape of input to predict\",batch_imgs_test.shape)\n                preds = model.predict(batch_imgs_test)\n                #print(\"the predicted y is of length : \", len(preds))\n                #print(\"first sample prediciton is \", preds[0])\n                ##classwise precision, recall \n                ##classwise precision, recall \n                acc_sum = 0\n                true_pos = 0\n                false_pos = 0\n                true_neg = 0\n                false_neg = 0\n                \n                class_true_pos = np.zeros([5,1])\n                class_true_neg = np.zeros([5,1])\n                class_false_pos = np.zeros([5,1])\n                class_false_neg = np.zeros([5,1])\n                \n                class_recall = np.zeros([5,1])\n                class_precision= np.zeros([5,1])\n                \n                \n                for i in range(len(preds)):\n                    for j in range(len(preds[0])):\n                        if(preds[i][j])>=0.5:\n                            preds[i][j] = 1\n                        if(preds[i][j]) < 0.5 :\n                            preds[i][j] = 0\n                for a in range(len(preds)):\n                    if(np.all(batch_labels_test[a] == preds[a])):\n                        acc_sum = acc_sum +1;\n                    for b in range(len(preds[a])):\n                        if(batch_labels_test[a][b] == preds[a][b] and preds[a][b] == 1):\n                           # acc_sum = acc_sum+1\n                            class_true_pos[b] = class_true_pos[b]+1\n                            true_pos = true_pos+1\n                            \n                        if(batch_labels_test[a][b] == preds[a][b] and preds[a][b] == 0):   \n                            class_true_neg[b] = class_true_neg[b]+1\n                            true_neg = true_neg+1\n                            \n                        if(batch_labels_test[a][b] < preds[a][b]):\n                            class_false_pos[b] = class_false_pos[b]+1\n                            false_pos = false_pos+1\n                            \n                        if(batch_labels_test[a][b] > preds[a][b]):\n                            class_false_neg[b] = class_false_neg[b]+1\n                            false_neg = false_neg+1\n                            \n                accuracy = (acc_sum/len(preds))*100\n                recall =    (true_pos) / (true_pos + false_neg)\n                precision =  (true_pos)/(true_pos + false_pos)\n                \n                for c in range(len(class_recall)):\n                    class_recall[c] = (class_true_pos[c]) / (class_true_pos[c] + class_false_neg[c])\n                    class_precision[c] = (class_true_pos[c]) / (class_true_pos[c] + class_false_pos[c])\n                    \n                print(\"Accuracy\",accuracy)\n                print(\"Recall\",recall)\n                print(\"Precision\",precision)\n                print(\"Class-wise precision\\n\", class_precision )\n                print(\"Class-wise recall\\n\", class_recall)\n                print(\"\\n\")\n                \n                self.dataloader.trigger_expire()\n                del batch_imgs_test\n                del batch_labels_test\n                processed +=self.split_size\n                        \n            return(preds,accuracy,recall,precision, class_recall, class_precision)\n                          ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#####MULTILABEL RUN#######\ndataloader = DataLoader(train_image_filepath)\nmodel = Basic(5,5)\ntrainer = ModelTrainer(dataloader,split_size=700)\nmodel = trainer.fit(X,y,model,epochs = 50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction , accuracy,recall,precision,class_recall, class_precision  = trainer.predict(X_test,y_test,model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy is :\",accuracy)\nprint(\"Class Wise Precision is :\", class_precision)\nprint(\"Class wise recall is :\", class_recall)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_model(model.model,\"epoch-model-four.hdf5\") ##works correctly\n#model.save(\"basic-model-1-epochs.h5\")\n#model.model.save_model(model,h5pyBasic-model-1-epoch)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}